{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "#print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#action_space: sin(theta),cos(theta) of both joints. Hence, size=4\n",
    "BUFFER_SIZE=100000\n",
    "BATCH_SIZE=128\n",
    "GAMMA=0.99\n",
    "TAU=0.001\n",
    "\n",
    "LRactor=5e-4\n",
    "LRcritic=5e-4\n",
    "UPDATE_EVERY=1\n",
    "\n",
    "#add seed to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,state_size=33, action_size=4,seed=0):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.fc1=nn.Linear(self.state_size,128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc2=nn.Linear(128,128)\n",
    "        self.fc3=nn.Linear(128,self.action_size)\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "    def forward(self, x):\n",
    "        #return F.tanh(self.fc3(F.relu(self.fc2(self.bn1(F.relu(self.fc1(x)))))))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        #x=self.bn1(x)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return F.tanh(x)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size=33,action_size=4,seed=0):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.fc1=nn.Linear(self.state_size,128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc2=nn.Linear(128+self.action_size,128)\n",
    "        self.fc3=nn.Linear(128,1)\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "    def forward(self,state,action):\n",
    "        x=F.relu(self.fc1(state))\n",
    "        #x=self.bn1(x)\n",
    "        #print(x.shape)\n",
    "        action=action.view(BATCH_SIZE,-1)\n",
    "        #print(action.shape)\n",
    "        x=torch.cat([x,action],1)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self,seed=0):\n",
    "        self.memory=deque(maxlen=BUFFER_SIZE)\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "    def add(self,state, action, reward,next_state,done):\n",
    "        self.memory.append([state,action,reward,next_state,done])\n",
    "    def sample(self,batch_size=BATCH_SIZE):\n",
    "        experiences=random.sample(self.memory, k=batch_size)\n",
    "        states=[]\n",
    "        actions=[]\n",
    "        rewards=[]\n",
    "        next_states=[]\n",
    "        dones=[]\n",
    "        for i in experiences:\n",
    "            states.append(i[0])\n",
    "            actions.append(i[1])\n",
    "            rewards.append(i[2])\n",
    "            next_states.append(i[3])\n",
    "            dones.append(i[4])\n",
    "        return states, actions,rewards,next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space=4, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000,seed=0):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = 4\n",
    "        self.low          = -1\n",
    "        self.high         = 1\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self,state_size=33,action_size=4,seed=0):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        \n",
    "        self.actorlocal=Actor(state_size,action_size).to(device)\n",
    "        self.actortarget=Actor(state_size,action_size).to(device)\n",
    "        \n",
    "        self.criticlocal=Critic(state_size,action_size).to(device)\n",
    "        self.critictarget=Critic(state_size,action_size).to(device)\n",
    "        \n",
    "        self.memory=Memory()\n",
    "        \n",
    "        self.noise=OUNoise(action_space=action_size)\n",
    "        \n",
    "        self.t_step=0\n",
    "        self.optimizeractor=optim.Adam(self.actorlocal.parameters(),lr=LRactor)\n",
    "        self.optimizercritic=optim.Adam(self.criticlocal.parameters(),lr=LRcritic)\n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "        self.hard_copy_weights(self.actortarget, self.actorlocal)\n",
    "        self.hard_copy_weights(self.critictarget, self.criticlocal)\n",
    "\n",
    "\n",
    "    \n",
    "    def hard_copy_weights(self, target, source):\n",
    "        \"\"\" copy weights from source to target network (part of initialization)\"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "    \n",
    "    def step(self,state, action,reward,next_state, done):\n",
    "        #self.memory.add(state, action,reward,next_state,done)\n",
    "        self.t_step=(self.t_step+1)%UPDATE_EVERY\n",
    "        if(self.t_step==0):\n",
    "            if(len(self.memory)>=BATCH_SIZE):\n",
    "                experiences=self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "    \n",
    "    def act(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action = self.actorlocal(state)\n",
    "        action = action.cpu().detach().numpy()\n",
    "        #return np.clip(action, -1, 1)\n",
    "        return action\n",
    "        \n",
    "        \n",
    "    \n",
    "    def learn(self,experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones=torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.criticlocal(states, actions)\n",
    "        next_actions = self.actortarget(next_states)\n",
    "        next_Q = self.critictarget(next_states, next_actions.detach())\n",
    "        rewards=rewards.view(BATCH_SIZE,1)\n",
    "        dones=dones.view(BATCH_SIZE,1)\n",
    "        Qprime = rewards + (GAMMA * next_Q *(1-dones))\n",
    "        #print(Qprime)\n",
    "        #print(Qvals)\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime.detach())\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss = -self.critictarget(states, self.actorlocal(states)).mean()\n",
    "        \n",
    "        # update networks\n",
    "        self.optimizeractor.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizeractor.step()\n",
    "\n",
    "        self.optimizercritic.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.optimizercritic.step()\n",
    "\n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actortarget.parameters(), self.actorlocal.parameters()):\n",
    "            target_param.data.copy_(param.data * TAU + target_param.data * (1.0 - TAU))\n",
    "       \n",
    "        for target_param, param in zip(self.critictarget.parameters(), self.criticlocal.parameters()):\n",
    "            target_param.data.copy_(param.data * TAU + target_param.data * (1.0 - TAU))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 81\tAverage Score: 30.02\n",
      "Environment solved in 81 episodes!\tAverage Score: 30.02\n"
     ]
    }
   ],
   "source": [
    "def reset(env):\n",
    "    env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "    return env_info.vector_observations\n",
    "\n",
    "def step(env,actions):\n",
    "    env_info = env.step(actions)[brain_name]       \n",
    "    next_states = env_info.vector_observations  \n",
    "    rewards = env_info.rewards                 \n",
    "    dones = env_info.local_done               \n",
    "    return next_states,rewards,dones\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_episodes=1000\n",
    "#eps=1\n",
    "#eps_decay=0.995\n",
    "#eps_final=0.01\n",
    "agent=DDPG()\n",
    "noise=OUNoise()\n",
    "scores_lst=[]\n",
    "scores_window=deque(maxlen=100)\n",
    "for i_episode in range(1,num_episodes+1):    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)\n",
    "    \n",
    "    t_step=0\n",
    "    agt_ctr=-1\n",
    "    while True:\n",
    "        actions=[noise.get_action(agent.act(states[j]),t_step) for j in range(20)]\n",
    "        env_info = env.step(actions)[brain_name]           \n",
    "        next_states = env_info.vector_observations         \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done\n",
    "        if(t_step%2==0):\n",
    "            agt_ctr=(agt_ctr+1)%20\n",
    "            agent.step(states[agt_ctr],actions[agt_ctr],rewards[agt_ctr],next_states[agt_ctr],dones[agt_ctr])\n",
    "        for j in range(20):\n",
    "            if(j!=agt_ctr):\n",
    "                agent.memory.add(states[j],actions[j], rewards[j],next_states[j],dones[j])        \n",
    "        scores += env_info.rewards\n",
    "        scores_lst.append(scores)\n",
    "        scores_window.append(scores)\n",
    "        states = next_states                               \n",
    "        t_step+=1\n",
    "        if np.any(dones):                                  \n",
    "            break\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    scores_lst.append(scores)\n",
    "    scores_window.append(scores)\n",
    "    if np.mean(scores_window)>30.0:\n",
    "        torch.save(agent.actorlocal.state_dict(), 'checkpointactor.pth')\n",
    "        torch.save(agent.criticlocal.state_dict(),'checkpointcritic.pth')\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Episode #')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb98621ddd8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np.mean(scores_lst,axis=1))), np.mean(scores_lst,axis=1))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 30.34699932169169\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions=[agent.act(states[j]) for j in range(20)]\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
