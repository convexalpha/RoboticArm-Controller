{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.8 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the State and Action Spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "BUFFER_SIZE=100000\n",
    "BATCH_SIZE=128\n",
    "GAMMA=0.99\n",
    "TAU=0.001\n",
    "\n",
    "LRactor=5e-4\n",
    "LRcritic=5e-4\n",
    "UPDATE_EVERY=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,state_size=33, action_size=4,seed=0):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.fc1=nn.Linear(self.state_size,128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc2=nn.Linear(128,128)\n",
    "        self.fc3=nn.Linear(128,self.action_size)\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return F.tanh(x)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size=33,action_size=4,seed=0):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.fc1=nn.Linear(self.state_size,128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc2=nn.Linear(128+self.action_size,128)\n",
    "        self.fc3=nn.Linear(128,1)\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "    def forward(self,state,action):\n",
    "        x=F.relu(self.fc1(state))\n",
    "        action=action.view(BATCH_SIZE,-1)\n",
    "        x=torch.cat([x,action],1)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self,seed=0):\n",
    "        self.memory=deque(maxlen=BUFFER_SIZE)\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "    def add(self,state, action, reward,next_state,done):\n",
    "        self.memory.append([state,action,reward,next_state,done])\n",
    "    def sample(self,batch_size=BATCH_SIZE):\n",
    "        experiences=random.sample(self.memory, k=batch_size)\n",
    "        states=[]\n",
    "        actions=[]\n",
    "        rewards=[]\n",
    "        next_states=[]\n",
    "        dones=[]\n",
    "        for i in experiences:\n",
    "            states.append(i[0])\n",
    "            actions.append(i[1])\n",
    "            rewards.append(i[2])\n",
    "            next_states.append(i[3])\n",
    "            dones.append(i[4])\n",
    "        return states, actions,rewards,next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ornsteinâ€“Uhlenbeck Noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space=4, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000,seed=0):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = 4\n",
    "        self.low          = -1\n",
    "        self.high         = 1\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self,state_size=33,action_size=4,seed=0):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        \n",
    "        self.actorlocal=Actor(state_size,action_size).to(device)\n",
    "        self.actortarget=Actor(state_size,action_size).to(device)\n",
    "        \n",
    "        self.criticlocal=Critic(state_size,action_size).to(device)\n",
    "        self.critictarget=Critic(state_size,action_size).to(device)\n",
    "        \n",
    "        self.memory=Memory()\n",
    "        \n",
    "        self.noise=OUNoise(action_space=action_size)\n",
    "        \n",
    "        self.t_step=0\n",
    "        self.optimizeractor=optim.Adam(self.actorlocal.parameters(),lr=LRactor)\n",
    "        self.optimizercritic=optim.Adam(self.criticlocal.parameters(),lr=LRcritic)\n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.seed=torch.manual_seed(seed)\n",
    "        self.hard_copy_weights(self.actortarget, self.actorlocal)\n",
    "        self.hard_copy_weights(self.critictarget, self.criticlocal)\n",
    "\n",
    "\n",
    "    \n",
    "    def hard_copy_weights(self, target, source):\n",
    "        \"\"\" copy weights from source to target network (part of initialization)\"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "    \n",
    "    def step(self,state, action,reward,next_state, done):\n",
    "        #self.memory.add(state, action,reward,next_state,done)\n",
    "        self.t_step=(self.t_step+1)%UPDATE_EVERY\n",
    "        if(self.t_step==0):\n",
    "            if(len(self.memory)>=BATCH_SIZE):\n",
    "                experiences=self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "    \n",
    "    def act(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action = self.actorlocal(state)\n",
    "        action = action.cpu().detach().numpy()\n",
    "        #return np.clip(action, -1, 1)\n",
    "        return action\n",
    "        \n",
    "        \n",
    "    \n",
    "    def learn(self,experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones=torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.criticlocal(states, actions)\n",
    "        next_actions = self.actortarget(next_states)\n",
    "        next_Q = self.critictarget(next_states, next_actions.detach())\n",
    "        rewards=rewards.view(BATCH_SIZE,1)\n",
    "        dones=dones.view(BATCH_SIZE,1)\n",
    "        Qprime = rewards + (GAMMA * next_Q *(1-dones))\n",
    "        #print(Qprime)\n",
    "        #print(Qvals)\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime.detach())\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss = -self.critictarget(states, self.actorlocal(states)).mean()\n",
    "        \n",
    "        # update networks\n",
    "        self.optimizeractor.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizeractor.step()\n",
    "\n",
    "        self.optimizercritic.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.optimizercritic.step()\n",
    "\n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actortarget.parameters(), self.actorlocal.parameters()):\n",
    "            target_param.data.copy_(param.data * TAU + target_param.data * (1.0 - TAU))\n",
    "       \n",
    "        for target_param, param in zip(self.critictarget.parameters(), self.criticlocal.parameters()):\n",
    "            target_param.data.copy_(param.data * TAU + target_param.data * (1.0 - TAU))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 94\tAverage Score: 32.66\n",
      "Environment solved in 94 episodes!\tAverage Score: 32.66\n"
     ]
    }
   ],
   "source": [
    "def reset(env):\n",
    "    env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "    return env_info.vector_observations\n",
    "\n",
    "def step(env,actions):\n",
    "    env_info = env.step(actions)[brain_name]       \n",
    "    next_states = env_info.vector_observations  \n",
    "    rewards = env_info.rewards                 \n",
    "    dones = env_info.local_done               \n",
    "    return next_states,rewards,dones\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_episodes=1000\n",
    "agent=DDPG()\n",
    "noise=OUNoise()\n",
    "scores_lst=[]\n",
    "scores_window=deque(maxlen=100)\n",
    "for i_episode in range(1,num_episodes+1):    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)\n",
    "    \n",
    "    t_step=0\n",
    "    agt_ctr=-1\n",
    "    while True:\n",
    "        actions=[noise.get_action(agent.act(states[j]),t_step) for j in range(20)]\n",
    "        env_info = env.step(actions)[brain_name]           \n",
    "        next_states = env_info.vector_observations         \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done\n",
    "        if(t_step%2==0):\n",
    "            agt_ctr=(agt_ctr+1)%20\n",
    "            agent.step(states[agt_ctr],actions[agt_ctr],rewards[agt_ctr],next_states[agt_ctr],dones[agt_ctr])\n",
    "        for j in range(20):\n",
    "            if(j!=agt_ctr):\n",
    "                agent.memory.add(states[j],actions[j], rewards[j],next_states[j],dones[j])        \n",
    "        scores += env_info.rewards\n",
    "        scores_lst.append(scores)\n",
    "        scores_window.append(scores)\n",
    "        states = next_states                               \n",
    "        t_step+=1\n",
    "        if np.any(dones):                                  \n",
    "            break\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    scores_lst.append(scores)\n",
    "    scores_window.append(scores)\n",
    "    if np.mean(scores_window)>30.0:\n",
    "        torch.save(agent.actorlocal.state_dict(), 'checkpointactor.pth')\n",
    "        torch.save(agent.criticlocal.state_dict(),'checkpointcritic.pth')\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Episode #')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7cfc1b69e8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np.mean(scores_lst,axis=1))), np.mean(scores_lst,axis=1))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 31.52499929536134\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions=[agent.act(states[j]) for j in range(20)]\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
